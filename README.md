# LangChain Ollama Streamlit Chatbot

This project is a local LLM-powered chatbot built using LangChain, Ollama (Gemma 2B model), and Streamlit.  
It demonstrates prompt chaining, output parsing, and LangSmith tracking integration.

## ğŸš€ Features
- Local LLM execution using Ollama
- Gemma 2B model integration
- Streamlit interactive UI
- Prompt templating with LangChain
- LangSmith tracing enabled
- Clean and minimal architecture

## ğŸ›  Tech Stack
- Python
- LangChain
- Ollama
- Streamlit
- Dotenv
  

## ğŸ“‚ Project Structure

```
langchain-ollama-streamlit-chatbot/
â”‚
â”œâ”€â”€ app.py              # Main Streamlit application
â”œâ”€â”€ req.txt             # Project dependencies
â”œâ”€â”€ .env                # Environment variables (API keys, project config) (not pushed to GitHub)
â”œâ”€â”€ .venv/              # Virtual environment (not pushed to GitHub)
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the Repository

```
git clone https://github.com/your-username/langchain-ollama-streamlit-chatbot.git
cd langchain-ollama-streamlit-chatbot
```

### 2ï¸âƒ£ Create Virtual Environment

```
python -m venv .venv
```

Activate it:

**Windows:**
```
.venv\Scripts\activate
```

**Mac/Linux:**
```
source .venv/bin/activate
```

### 3ï¸âƒ£ Install Dependencies

```
pip install -r req.txt
```

### 4ï¸âƒ£ Setup Environment Variables

Create a `.env` file and add:

```
LANGCHAIN_API_KEY=your_key_here
LANGCHAIN_PROJECT=your_project_name
```

---

## â–¶ï¸ Run the Application

Make sure Ollama is running locally.

```
streamlit run app.py
```

---



## ğŸ‘¨â€ğŸ’» Author

Sangati Daveedu

---
